---
title: Insert title here
key: 94b2bb832f5e9fe7d78c69b3c169956e

---
## Least Squares Approximation

```yaml
type: "TitleSlide"
key: "24d58dfa91"
```

`@lower_third`

name: Dan Golding
title: Data Scientist


`@script`
In the previous section we used the matrix inverse to solve a system of linear equations with exactly one solution. In this section we’ll use linear algebra to find the best approximation in situations without exact solutions.


---
## Consistency

```yaml
type: "TwoColumns"
key: "396d9564b5"
center_content: false
```

`@part1`
Two equations with two unknowns:

$$
\begin{array}{}
3x & - & y & = -1 \\\
0.5x &-& y & = -4 \\\
\end{array}
$$

Can be represented as a matrix-vector equation{{1}}

$$\left[ {\begin{array}{cc} 3 & -1 \\\ 0.5 & -1 \end{array} } \right] \left[ {\begin{array}{c} x \\\ y \end{array} } \right] = \left[ {\begin{array}{c} -1 \\\ -4 \end{array} } \right]$${{1}}

or{{1}}

$$\textbf{A}\vec{x}=\vec{b}$${{1}}

$$\therefore \vec{x}=\textbf{A}^{-1}\vec{b}$${{2}}


`@part2`
We can visualise this system as two intersecting straight lines{{3}}

![Graphical representation of consistent system of 2 equations with 2 unknowns ](https://assets.datacamp.com/production/repositories/4435/datasets/d2ca724f690bc19d93d9f67203aff94240fae569/two_by_two_consistent.png){{3}}


`@script`
We have seen that a system of two equations with two unknowns, can be represented as a matrix vector equation, that's solved using the matrix inverse. Graphically, this is two intersecting lines, with x being the point of intersection. 

This only works when A is invertible. We call a system like this consistent.


---
## Inconsistent systems

```yaml
type: "TwoRowsTwoColumns"
key: "06c9151650"
```

`@part1`
Two equations, two unknowns {{1}}

![](https://assets.datacamp.com/production/repositories/4435/datasets/f0c9cec7fa42c9c573d562f9e6009602d8b36635/two_by_two_inconsistent.png){{1}}


`@part2`
Three equations, two unknowns{{3}}

![](https://assets.datacamp.com/production/repositories/4435/datasets/5426fbdf4b1de6be72892c216eec07b3ded7be37/three_by_two_inconsistent.png){{3}}


`@part3`
$$\left[ {\begin{array}{cc} 3 & -1 \\\ 1.5 & -0.5 \end{array} } \right] \left[ {\begin{array}{c} x \\\ y \end{array} } \right] = \left[ {\begin{array}{c} -1 \\\ -2.5 \end{array} } \right]$${{2}}


Note the bottom row is a constant multiple of the top row{{2}}


`@part4`
$$\left[ {\begin{array}{cc} 3 & -1 \\\ 0.5 & -1 \\\ -1 & -1 \end{array} } \right] \left[ {\begin{array}{c} x \\\ y \end{array} } \right] = \left[ {\begin{array}{c} -1 \\\ -4 \\\ -5 \end{array}} \right]$${{4}}

When there are more equations than unknowns, we call the system _overdetermined_{{4}}


`@script`
However, not all systems are invertible. In such a case, there is no solution. We call such a system inconsistent.

Consider the case of two parallel lines. This is still a system of two equations with two unknowns. However in this case, the matrix A is not invertible. In this example, the top row is two times the bottom row.

What about a system of three straight lines? Here we have three equations, but still only two unknowns. We see from the graph that there is no point where all three lines intersect. A system with more equations than unknowns usually does not have an exact solution. We call such systems overdetermined.


---
## An approximate solution

```yaml
type: "TwoColumns"
key: "e2ba674900"
disable_transition: false
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/5426fbdf4b1de6be72892c216eec07b3ded7be37/three_by_two_inconsistent.png)


`@part2`



`@script`
In data science, overdetermined systems are very common. Although we can't find an exact solution a useful approximation...


---
## An approximate solution

```yaml
type: "TwoColumns"
key: "29d09bf6f5"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/09ea633d651a83a1d747d869e16c15512d73824d/three_by_two_inconsistent_solved.png)


`@part2`
The psuedoinverse, denoted $A^{+}$, has the property: $\textbf{A}^{+}\textbf{A}\approx\textbf{I}${{1}}

but not the other way around, i.e.: $\textbf{A}\textbf{A}^{+}\neq\textbf{I}${{1}}


`@script`
...is the point that minimises the squared distance between each line. This is the least squares approximation.

One of many methods for solving the least squares problem is the Moore-Pensrose psuedoinverse. The psdeuoinverse of A, A plus, has the familiar property that A plus multiplied with A is the identity matrix, or as close as possible.


---
## An approximate solution

```yaml
type: "TwoColumns"
key: "39b78d3377"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/09ea633d651a83a1d747d869e16c15512d73824d/three_by_two_inconsistent_solved.png)


`@part2`
The psuedoinverse, denoted $A^{+}$, has the property: $\textbf{A}^{+}\textbf{A}\approx\textbf{I}$

but not the other way around, i.e.: $\textbf{A}\textbf{A}^{+}\neq\textbf{I}$

We can use the psuedoinverse to solve for the least squares approximation (denoted $\vec{x}^{*})$:

$$\textbf{A}\vec{x}^{\*}\approx\vec{b}$$
$$\textbf{A}^{+}\textbf{A}\vec{x}^{\*}=\textbf{A}^{+}\vec{b}$$
$$\textbf{I}\vec{x}^{\*}=\textbf{A}^{+}\vec{b}$$
$$\vec{x}^{*}=\textbf{A}^{+}\vec{b}$$


`@script`
We use the pseudoinverse to solve for the least squares approximation the same way we used the inverse to solve consistent systems


---
## Psuedoinverse in python

```yaml
type: "TwoColumns"
key: "5ed7bfadfc"
```

`@part1`
We can find the psuedoinverse using the `pinv` function in numpy

```python
A_plus = np.linalg.pinv(A)
```

let's use numpy to verify the properties of $\textbf{A}^{+}$


`@part2`
```python
A = np.array([[3.0,-1],
              [0.5,-1],
              [-3, -1],])
A_plus = np.linalg.pinv(A)

print(A_plus@A)
print(A@A_plus)
```{{2}}

we get{{3}}

```python
[[1.00000000e+00 0.00000000e+00]
 [2.22044605e-16 1.00000000e+00]] 

[[ 0.77523 0.38532 -0.16055]
 [ 0.38532 0.33945  0.27523]
 [-0.16055 0.27523  0.88532]] 
```{{3}}


`@script`
We can calculate the psuedoinverse in Python using numpy’s pinv function

Let's use numpy to verify the properties of the pseudoinverse

We create a 3-by-2 matrix A and find its pseudoinverse, A_plus. 

We see that A_plus times A is very close to the identity matrix. However A times A_plus is completely different.


---
## Ordinary least squares regression

```yaml
type: "TwoColumns"
key: "7956d47260"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/a6deeb0c00cbec45494eb24ba69185c3409e6118/random_data.png){{1}}

This chart shows data of the form $y=3-2x$ with random noise added to it{{1}}


`@part2`
The goal is to solve for $\min_{\beta}||y-f(x)||${{2}}

where $f(x)=\beta_{0}+\beta^{1} x${{2}}

We can represent this as system of linear equations:{{3}}

$$\textbf{X}\vec{\beta}=\vec{y}$${{3}}

with one equation per data point{{4}}


$$\left[ {\begin{array}{cc} 1 & x_{0} \\\ 1 & x{1} \\\ \vdots & \vdots \\\ 1 & x{n} \end{array} } \right] \left[ {\begin{array}{c} \beta{0} \\\ \beta{1} \end{array} } \right] = \left[ {\begin{array}{c} y{0} \\\ y{1} \\\ \vdots \\\ y{n} \end{array} } \right] $${{4}}


`@script`
A common task in machine learning is building a model from data.

These data were created using a linear system given by 3 - 2x, but with a lot of noise. How can we use the linear algebra techniques we've learned to model these noisy observations?

Formally, we want to minimise the squared difference between the observations, y, and the modelled values, f(x). We use a linear model, f(x) = beta zero + beta one x and find those betas that give the least squared error.

We use a matrix-vector equation, X beta = y, which we have seen represents a system of linear equations. We have two unknowns, the elements of the vector beta, but many more equations, one for each data-point. This system is overdetermined thus has no exact solution. As with the three intersecting lines, we can find an approximate, beta star, solution minimising the squared error between the model and each data point.


---
## Ordinary least squares regression in python

```yaml
type: "TwoColumns"
key: "39c4bb3311"
```

`@part1`
$$\textbf{X}\vec{\beta}^{*}\approx\vec{y}$$

$$\therefore\vec{\beta}^{*}=\textbf{X}^{+}\vec{y}$$

```python
B_star = np.linalg.pinv(X)@y
```{{1}}

which gives us a `B_star` of{{2}}

```python
array([[2.83866],
       [-2.04398]])
```{{2}}

$$\vec{\beta} = \left[ {\begin{array}{c} \\; \enspace 3 \\\ -2 \end{array} } \right] \qquad \vec{\beta}^{*}= \left[ {\begin{array}{c} \\; \enspace 2.84 \\\ -2.04 \end{array} } \right]$${{2}}


`@part2`
![](https://assets.datacamp.com/production/repositories/4435/datasets/8551476101d6cc6cfd67fe6951f6834f760152a4/OLS.png){{3}}


`@script`
We use the pseudoinverse, X plus, to find beta star.

using pinv

we get the answer 2.84 for beta 0 and -2.04 for beta 1. Using our knowledge of linear algebra, and a single line of code, we have managed to find the model used to create data almost exactly , even in the presence of heavy noise

The green line shows the real data model. The red line, the least squares model, barely deviates from it.


---
## Let's Practice

```yaml
type: "FinalSlide"
key: "b64609a939"
```

`@script`
Let's try this in action

