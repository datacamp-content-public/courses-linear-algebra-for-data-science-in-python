---
title: Insert title here
key: 94b2bb832f5e9fe7d78c69b3c169956e

---
## Least Squares Approximation

```yaml
type: "TitleSlide"
key: "24d58dfa91"
```

`@lower_third`

name: Dan Golding
title: undefined


`@script`
In the previous section we saw how we can use a matrix inverse to solve a system of linear equations that has exactly one solution. In this section we will look at situations without perfect solutions and how we can use linear algebra to find the best approximation.


---
## Consistency

```yaml
type: "TwoColumns"
key: "396d9564b5"
center_content: false
```

`@part1`
Two equations with two unknowns:

$$
\begin{array}{}
3x & - & y & = -1 \\\
0.5x &-& y & = -4 \\\
\end{array}
$$

Can be represented as a matrix-vector equation{{1}}

$$
\left[ {\begin{array}{cc}
3 & -1 \\\
0.5 & -1
\end{array} } \right]
\left[ {\begin{array}{c}
x \\\
y
\end{array} } \right]
=
  \left[ {\begin{array}{c}
   -1 \\\
   -4
  \end{array} } \right]
$${{1}}

or{{1}}

$$
\textbf{A}\vec{x}=\vec{b}
$${{1}}


`@part2`
We can visualise this system as two intersecting lines{{2}}

![Graphical representation of consistent system of 2 equations with 2 unknowns ](https://assets.datacamp.com/production/repositories/4435/datasets/d2ca724f690bc19d93d9f67203aff94240fae569/two_by_two_consistent.png){{2}}


`@script`
We have that given a system of two equations with two unknowns, {{1}} we can represent it as a matrix vector equation. {{2}} Graphically, we see two intersecting straight lines. 

However, this is only the case if the matrix A is invertible. We call a system like this consistent.


---
## Inconsistent systems

```yaml
type: "TwoRowsTwoColumns"
key: "06c9151650"
```

`@part1`
Two equations, two unknowns {{1}}

![](https://assets.datacamp.com/production/repositories/4435/datasets/f0c9cec7fa42c9c573d562f9e6009602d8b36635/two_by_two_inconsistent.png){{1}}


`@part2`
Three equations, two unknowns{{3}}

![](https://assets.datacamp.com/production/repositories/4435/datasets/5426fbdf4b1de6be72892c216eec07b3ded7be37/three_by_two_inconsistent.png){{3}}


`@part3`
$$
\left[ {\begin{array}{cc}
3 & -1 \\\
1.5 & -0.5
\end{array} } \right]
\left[ {\begin{array}{c}
x \\\
y
\end{array} } \right]
=
  \left[ {\begin{array}{c}
   -1 \\\
   -2.5
  \end{array} } \right]
$${{2}}


Note the bottom row is a constant multiple of the top row{{2}}


`@part4`
$$
\left[ {\begin{array}{cc}
3 & -1 \\\
0.5 & -1 \\\
-1 & -1
\end{array} } \right]
\left[ {\begin{array}{c}
x \\\
y
\end{array} } \right]
=
  \left[ {\begin{array}{c}
   -1 \\\
   -4 \\\
   -5
  \end{array} } \right]
$${{4}}

When there are more equations than unknowns we call the system _overdetermined_{{4}}


`@script`
However, not all systems are invertible. In such a case, there is no solution to the system and we call such a system inconsistent.

{{1}} Consider the case of two parallel lines. We can still represent this as a system of two equations with two unknowns. {{2}} However when we look at the matrix A, we find it is not invertible. In this example, you can see that the top row is two times the bottom row.

{{3}} Next consider a system of three straight lines. In this case, we have three equations, but still only two unknowns. You can see from the graph that there is no point where all three lines intersect. Such a system, with more equations than unknowns, usually does not have an exact solution and we call such a system overdetermined.


---
## An approximate solution

```yaml
type: "TwoColumns"
key: "e2ba674900"
disable_transition: false
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/5426fbdf4b1de6be72892c216eec07b3ded7be37/three_by_two_inconsistent.png)


`@part2`



`@script`
In data science, you are far more likely to encounter an overdetermined system than a consistent one. Although we can't find an exact solution...


---
## An approximate solution

```yaml
type: "TwoColumns"
key: "29d09bf6f5"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/09ea633d651a83a1d747d869e16c15512d73824d/three_by_two_inconsistent_solved.png)


`@part2`
The psuedoinverse, denoted $A^{+}$, has the property: $\textbf{A}^{+}\textbf{A}\approx\textbf{I}${{1}}

but not the other way around, i.e.: $\textbf{A}\textbf{A}^{+}\neq\textbf{I}${{1}}

We can use the psuedoinverse to solve for the least squares approximation (denoted $\vec{x}^{*})$:{{2}}

$$\textbf{A}\vec{x}\approx \vec{b}$$
$$\textbf{A}^{+}\textbf{A}\vec{x}^{\*}=\textbf{A}^{+}\vec{b}$$
$$\textbf{I}\vec{x}^{\*}=\textbf{A}^{+}\vec{b}$$
$$\vec{x}^{*}=\textbf{A}^{+}\vec{b}$${{2}}


`@script`
...a useful approximation is point that minimises the squared distance between the lines. We call this approximation the least squares solution.

There are many methods for solving the least squares problem, for example using the moore-pensrose psuedoinverse. The psdeuoinverse of A, denoted A plus, has the familiar property that A plus A is the identity matrix, or as close to it as possible. 

This lets us use it to solve the matrix-vector equation as before.

The vector x star is the point shown in red on the graph. This is the least squares approximate solution.


---
## Psuedoinverse in python

```yaml
type: "TwoColumns"
key: "5ed7bfadfc"
```

`@part1`
Given our system

$$
\left[ {\begin{array}{cc}
3 & -1 \\\
0.5 & -1 \\\
-1 & -1
\end{array} } \right]
\left[ {\begin{array}{c}
x \\\
y
\end{array} } \right]
=
  \left[ {\begin{array}{c}
   -1 \\\
   -4 \\\
   -5
  \end{array} } \right]
$$

we can find the psuedoinverse using the `pinv` function in numpy{{1}}

```python
A_plus = np.linalg.pinv(A)
```{{1}}

let's use numpy to verify the properties of $\textbf{A}^{+}${{2}}


`@part2`
```python
A = np.array([[3.0,-1],
              [0.5,-1],
              [-3, -1],])

A_plus = np.linalg.pinv(A)

print(A_plus@A)
print(A@A_plus)
```{{3}}

we get{{4}}

```python
[[1.00000000e+00 0.00000000e+00]
 [2.22044605e-16 1.00000000e+00]] 

[[ 0.77523 0.38532 -0.16055]
 [ 0.38532 0.33945  0.27523]
 [-0.16055 0.27523  0.88532]] 
```{{4}}


`@script`



---
## Ordinary least squares regression

```yaml
type: "TwoColumns"
key: "7956d47260"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4435/datasets/88b4a1e941d5aeeaaa443bd69c6139b84753315f/random_data.png)

Note this chart shows data of the form $y=3-2x$ with random noise added to it


`@part2`
The goal is to solve for $\min_{\beta}||y-f(x)||$

where $f(x)=\beta_{0}+\beta^{1} x$ 

Given a set of observations, we can represent this as an overdetermined system of linear equations:


$$
\left[ {\begin{array}{cc}
1 & x_{0} \\\
1 & x{1} \\\
\vdots & \vdots \\\
1 & x{n}
\end{array} } \right]
\left[ {\begin{array}{c}
\beta{0} \\\
\beta{1}
\end{array} } \right]
=
\left[ {\begin{array}{c}
y{0} \\\
y{1} \\\
\vdots \\\
y{n}
\end{array} } \right]
$$

Or simply
$$\textbf{X}\vec{\beta}=\vec{y}$$


`@script`
A very common data science use case of solving an overdetermined system is building a predictive model from observed data.

For example consider these data, they represent a linear system given by 3 - 2x, however there is a lot of noise in the observations. We can use the linear algebra techniques to build a model from these noisy observations that will closely approximate 3 - 2x.

{{1}} Formally, we say we want to minimise the squared difference between the observations, y, and the model predictions, f(x). We have chosen a simple linear model, f(x) = beta zero + beta one x and we want to find the betas that give us the least squares approximation.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "b64609a939"
```

`@script`


